{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ai.Client()\n",
    "client.configure({\n",
    "  \"ollama\" : {\n",
    "    \"timeout\": 600,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `aisuite` with dummy prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Respond in Pirate English. Always try to include the phrase - No rum No fun.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about Captain Jack Sparrow\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED          \n",
      "phi4:latest        ac896e5b8b34    9.1 GB    55 seconds ago       \n",
      "gemma2:9b          ff02c3702f32    5.4 GB    41 minutes ago       \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    About an hour ago    \n",
      "deepseek-r1:8b     28f8fd6cdc67    4.9 GB    2 hours ago          \n",
      "llama3.3:latest    a6eb4748fd29    42 GB     3 weeks ago          \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    #\"ollama:deepseek-r1:8b\",\n",
    "    #\"ollama:llama:3.3:latest\",\n",
    "    \"ollama:llama3.2:latest\",\n",
    "    \"ollama:gemma2:9b\",\n",
    "    \"ollama:phi4:latest\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = {}\n",
    "\n",
    "for selected_model in models:\n",
    "    response = client.chat.completions.create(model=selected_model, messages=messages)\n",
    "    replies[selected_model] = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ollama:llama3.2:latest; reply's length = 243\n",
      "Model: ollama:gemma2:9b; reply's length = 304\n"
     ]
    }
   ],
   "source": [
    "for k,v in replies.items():\n",
    "    print(f\"Model: {k}; reply's length = {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ollama:llama3.2:latest': \"Yer lookin' fer a joke about that scurvy dog, eh? Alright then, listen close:\\n\\nWhy did Captain Jack Sparrow bring a ladder aboard his ship?\\n\\nBecause he heard the drinks were on the house! Arrr, no rum, no fun!\",\n",
       " 'ollama:gemma2:9b': \"Ahoy, matey! Ye want a tale 'bout ol' Jack Sparrow? \\n\\nGather 'round and listen close:\\n\\nWhy did Captain Jack Sparrow always carry two compasses? \\n\\nTo be sure he wasn't lost at sea...and to have one to point the way to the nearest grog stash! No rum, no fun, ye hear?  🍻💀\\n\\n\\n\",\n",
       " 'ollama:phi4:latest': 'Ahoy there, matey! Gather \\'round for a tale o\\' ol\\' Cap\\'n Jack Sparrow!\\n\\nSo, what happens when you mix Captain Jack Sparrow with a chicken?\\n\\nYou get... \"Cluckin\\' up the wrong ship!\"\\n\\nArrr, no rum, no fun! But remember, ye never know where yer adventure will take ye next!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query LLMs with real TextEnt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for each document, load the pre-generated summary\n",
    "- based on the summary, for each doc generate 3 prompts (metadata, metadata + incipit, metadata + summary)\n",
    "- iterate over doc, iterate over prompts per doc, iterate over models, then query with triples (docu, model, prompt)\n",
    "\n",
    "- start with a spacy document\n",
    "- load the corresponding pre-generated summary\n",
    "- define a `build_prompts` function that takes a `spacy_doc` as input and returns a list of tuples `('prompt-id', 'prompt-message')` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def build_summary_prompt(spacy_doc: Doc) -> str:\n",
    "    \"\"\"\n",
    "    Builds a summary prompt based on a spaCy document.\n",
    "\n",
    "    Args:\n",
    "        spacy_doc (Doc): A spaCy document object containing the text and metadata.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted summary prompt.\n",
    "\n",
    "    The summary is loaded from a JSON file located in the \"../data/summaries\" directory.\n",
    "    The filename of the summary is derived from the 'document_id' stored in the user_data attribute of the spaCy document.\n",
    "    \"\"\"\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "\n",
    "    # load base prompt\n",
    "    with open(\"../data/prompts/summary_prompt.txt\", \"r\") as file:\n",
    "        base_prompt = file.read()\n",
    "\n",
    "    # load the pre-computed summary from its JSON file\n",
    "    with doc_summary_path.open('r', encoding='utf-8') as file:\n",
    "        summary = json.load(file)\n",
    "\n",
    "    # JSON to pretty string\n",
    "    summary_as_string = json.dumps(summary, indent=2, ensure_ascii=False)\n",
    "    return base_prompt.format(document_summary=summary_as_string)\n",
    "\n",
    "def build_incipit_prompt(spacy_doc: Doc) -> str:\n",
    "    pass\n",
    "\n",
    "def build_prompts(spacy_doc: Doc) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Builds prompts based on a spaCy document.\n",
    "\n",
    "    Args:\n",
    "        spacy_doc (Doc): A spaCy document object containing the text and metadata.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: A list of tuples where each tuple contains a prompt ID and its text.    \n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    prompts.append(\n",
    "          ('prompt-w-summary', build_summary_prompt(spacy_doc)),\n",
    "          ('prompt-incipit', build_incipit_prompt(spacy_doc)),\n",
    "    )\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_generate_prompts(spacy_docs: List[Doc], output_path: Path) -> None:\n",
    "    for spacy_doc in tqdm(spacy_docs, desc=\"Pre-generating prompts\"):\n",
    "        doc_id = spacy_doc.user_data[\"document_id\"]\n",
    "        prompts = build_prompts(spacy_doc)\n",
    "\n",
    "        # Define the path to the directory\n",
    "        directory_path = output_path / doc_id\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not directory_path.exists():\n",
    "            directory_path.mkdir(parents=True, exist_ok=True) # Create the directory if it does not exist\n",
    "\n",
    "        for prompt_id, prompt in prompts:\n",
    "            print(f\"Writing prompt {prompt_id} for document {doc_id}\")\n",
    "            with open(output_path / doc_id / f\"{doc_id}_{prompt_id}.txt\", \"w\") as file:\n",
    "                file.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from textentlib.utils import load_or_create_corpus, nlp_model_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_CORPUS_SERIALIZED_PATH = Path(\"../data/corpus_24012025.spacy\")\n",
    "PRE_GENERATED_PROMPTS_PATH = Path(\"../data/prompts/pregenerated\")    \n",
    "SAMPLE_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded serialize spacy corpus from ../data/corpus_24012025.spacy\n",
      "Number of documents in the corpus: 594\n",
      "Number of entities in the corpus: 287389\n",
      "Number of tokens in the corpus: 12885306\n"
     ]
    }
   ],
   "source": [
    "spacy_corpus = load_or_create_corpus(SPACY_CORPUS_SERIALIZED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = spacy_corpus.get_docs(nlp_model_fr.vocab)\n",
    "docs = list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - we may want to exclude documents in the validation set\n",
    "# - we may want to exclude documents that are very long (> 150k tokens)\n",
    "sampled_docs = random.sample(docs, SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-generating prompts: 100%|██████████| 10/10 [00:00<00:00, 258.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prompt prompt-w-summary for document bpt6k1090087k\n",
      "Writing prompt prompt-w-summary for document bpt6k10900457\n",
      "Writing prompt prompt-w-summary for document bpt6k1280456m\n",
      "Writing prompt prompt-w-summary for document bpt6k1090099s\n",
      "Writing prompt prompt-w-summary for document bpt6k1280403g\n",
      "Writing prompt prompt-w-summary for document bpt6k1090071q\n",
      "Writing prompt prompt-w-summary for document bpt6k56266087\n",
      "Writing prompt prompt-w-summary for document bpt6k10901460\n",
      "Writing prompt prompt-w-summary for document bpt6k1521653f\n",
      "Writing prompt prompt-w-summary for document bpt6k56285481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pre_generate_prompts(sampled_docs, PRE_GENERATED_PROMPTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
